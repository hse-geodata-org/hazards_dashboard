{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61ff899b",
   "metadata": {},
   "source": [
    "Код расчитан на то, что шейпы называются как в gdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd46f2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcpy\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import log10, floor, isfinite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362d04be",
   "metadata": {},
   "source": [
    "Параметры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2751511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sdfsg\\Downloads\\Yakutia_RiskProfile_10-10-2025\\Yakutia_RiskProfile_08-10-2025\\Yakutia_RiskProfile\\Default.gdb\\risk\n",
      "['Yakutia_hazardTemplate_MO', 'Yakutia_hazHeat_MO', 'Yakutia_hazFreez5_MO', 'Yakutia_hazDrought_MO', 'Yakutia_hazThunderstorm_MO', 'Yakutia_hazHail_MO', 'Yakutia_hazRain_MO', 'Yakutia_hazWind_MO', 'Yakutia_hazCold_MO', 'Yakutia_hazFlood_MO', 'Yakutia_hazPermafrost_MO', 'Yakutia_hazWildfire_MO']\n",
      "template\n",
      "C:\\Users\\sdfsg\\Downloads\\Yakutia_RiskProfile_10-10-2025\\Yakutia_RiskProfile_08-10-2025\\Yakutia_RiskProfile\\Default.gdb\\risk\\Yakutia_hazHeat_MO\n",
      "C:\\Users\\sdfsg\\Downloads\\Yakutia_RiskProfile_10-10-2025\\Yakutia_RiskProfile_08-10-2025\\Yakutia_RiskProfile\\Default.gdb\\risk\\Yakutia_hazFreez5_MO\n",
      "C:\\Users\\sdfsg\\Downloads\\Yakutia_RiskProfile_10-10-2025\\Yakutia_RiskProfile_08-10-2025\\Yakutia_RiskProfile\\Default.gdb\\risk\\Yakutia_hazDrought_MO\n",
      "C:\\Users\\sdfsg\\Downloads\\Yakutia_RiskProfile_10-10-2025\\Yakutia_RiskProfile_08-10-2025\\Yakutia_RiskProfile\\Default.gdb\\risk\\Yakutia_hazThunderstorm_MO\n",
      "C:\\Users\\sdfsg\\Downloads\\Yakutia_RiskProfile_10-10-2025\\Yakutia_RiskProfile_08-10-2025\\Yakutia_RiskProfile\\Default.gdb\\risk\\Yakutia_hazHail_MO\n",
      "C:\\Users\\sdfsg\\Downloads\\Yakutia_RiskProfile_10-10-2025\\Yakutia_RiskProfile_08-10-2025\\Yakutia_RiskProfile\\Default.gdb\\risk\\Yakutia_hazRain_MO\n",
      "C:\\Users\\sdfsg\\Downloads\\Yakutia_RiskProfile_10-10-2025\\Yakutia_RiskProfile_08-10-2025\\Yakutia_RiskProfile\\Default.gdb\\risk\\Yakutia_hazWind_MO\n",
      "C:\\Users\\sdfsg\\Downloads\\Yakutia_RiskProfile_10-10-2025\\Yakutia_RiskProfile_08-10-2025\\Yakutia_RiskProfile\\Default.gdb\\risk\\Yakutia_hazCold_MO\n",
      "C:\\Users\\sdfsg\\Downloads\\Yakutia_RiskProfile_10-10-2025\\Yakutia_RiskProfile_08-10-2025\\Yakutia_RiskProfile\\Default.gdb\\risk\\Yakutia_hazFlood_MO\n",
      "C:\\Users\\sdfsg\\Downloads\\Yakutia_RiskProfile_10-10-2025\\Yakutia_RiskProfile_08-10-2025\\Yakutia_RiskProfile\\Default.gdb\\risk\\Yakutia_hazPermafrost_MO\n",
      "C:\\Users\\sdfsg\\Downloads\\Yakutia_RiskProfile_10-10-2025\\Yakutia_RiskProfile_08-10-2025\\Yakutia_RiskProfile\\Default.gdb\\risk\\Yakutia_hazWildfire_MO\n",
      "C:\\Users\\sdfsg\\Downloads\\Yakutia_RiskProfile_10-10-2025\\Yakutia_RiskProfile_08-10-2025\\Yakutia_RiskProfile\\Default.gdb\\economics\\Yakutia_economics_MO\n",
      "Hazards loaded: ['Heat', 'Freeze', 'Drought', 'Thunder', 'Hail', 'Precipitation', 'Wind', 'Cold', 'Flood', 'Permafrost', 'Wildfire', 'Economy']\n",
      "\n",
      "Total asset values (from Economy):\n",
      "  ho: 2,913,685,825,408\n",
      "  ro: 2,746,655,274,432\n",
      "  fo: 38,935,168,449,916\n",
      "  ag: 1,640,855,360\n",
      "  pop: 994,178\n"
     ]
    }
   ],
   "source": [
    "# ========= PATHS =========\n",
    "region ='Yakutia'\n",
    "gdb_path   = r\"C:\\Users\\sdfsg\\Downloads\\Yakutia_RiskProfile_10-10-2025\\Yakutia_RiskProfile_08-10-2025\\Yakutia_RiskProfile\\Default.gdb\"\n",
    "RISKS_FDS  = \"risk\"       # hazards live here\n",
    "ECON_FDS   = \"economics\"   # Economy lives here\n",
    "ECON_FC    = f\"{region}_economics_MO\" \n",
    "\n",
    "# ========= MAPPING & CONFIG =========\n",
    "hazard_mapping = {\n",
    "    \"economics\": \"Economy\",\n",
    "    \"hazCold\": \"Cold\",\n",
    "    \"hazDrought\": \"Drought\",\n",
    "    \"hazWildfire\": \"Wildfire\",\n",
    "    \"hazFlood\": \"Flood\",\n",
    "    \"hazFreez5\": \"Freeze\",\n",
    "    \"hazHail\": \"Hail\",\n",
    "    \"hazHeat\": \"Heat\",\n",
    "    \"hazRain\": \"Precipitation\",     # (spelling fixed)\n",
    "    \"hazThunderstorm\": \"Thunder\",\n",
    "    \"hazWind\": \"Wind\",\n",
    "    \"hazPermafrost\": \"Permafrost\",\n",
    "}\n",
    "\n",
    "category_order = [\"неопасно\",\"умерено опасно\",\"опасно\",\"весьма опасно\",\"очень опасно\"]\n",
    "asset_types = [\"ho\", \"ro\", \"fo\", \"ag\", \"pop\"]\n",
    "\n",
    "# ========= HELPERS =========\n",
    "def extract_short_code(fc_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Try to pull a 'hazXXXX' token from the FC name.\n",
    "    Example: 'Yakutia_hazCold_ExportFeature' -> 'hazCold'\n",
    "    Fallback: if a known key is a substring of the name, use that.\n",
    "    Else: return the whole name.\n",
    "    \"\"\"\n",
    "    m = re.search(r\"(haz[A-Za-z0-9]+)\", fc_name)\n",
    "    if m:\n",
    "        return m.group(1)\n",
    "    for key in hazard_mapping.keys():\n",
    "        if key.lower() in fc_name.lower():\n",
    "            return key\n",
    "    return fc_name\n",
    "\n",
    "# ========= COLLECT HAZARDS FROM GDB =========\n",
    "hazard_data = {}  # dict[str, str] => { \"Wildfire\": \"<full path to FC>\", ... }\n",
    "\n",
    "# 1) Hazards inside RISKS_FDS\n",
    "arcpy.env.workspace = os.path.join(gdb_path, RISKS_FDS)\n",
    "print(arcpy.env.workspace)\n",
    "print(arcpy.ListFeatureClasses())\n",
    "for fc in arcpy.ListFeatureClasses() or []:\n",
    "    name_l = fc.lower()\n",
    "    if \"template\" in name_l or \"perm_hazardtemplate_mo\" in name_l:\n",
    "        print('template')\n",
    "        continue  # skip your template FC\n",
    "\n",
    "    short = extract_short_code(fc)\n",
    "    hazard_name = hazard_mapping.get(short, short)\n",
    "    full_path = os.path.join(gdb_path, RISKS_FDS, fc)\n",
    "    print(full_path)\n",
    "    hazard_data[hazard_name] = full_path\n",
    "\n",
    "# 2) Economy from ECON_FDS (explicit FC name you provided)\n",
    "econ_path = os.path.join(gdb_path, ECON_FDS, ECON_FC)\n",
    "print(econ_path)\n",
    "if arcpy.Exists(econ_path):\n",
    "    hazard_data[\"Economy\"] = econ_path\n",
    "else:\n",
    "    print(f\"⚠️ Economy feature class not found at: {econ_path}\")\n",
    "\n",
    "print(\"Hazards loaded:\", list(hazard_data.keys()))\n",
    "\n",
    "# ========= TOTALS FROM ECONOMY =========\n",
    "total_prices = {}\n",
    "if \"Economy\" in hazard_data:\n",
    "    # use your actual field names from the field listing\n",
    "    econ_fields = {\n",
    "        \"ho\": \"hoPriceTot\",\n",
    "        \"ro\": \"roPriceTot\",\n",
    "        \"fo\": \"foPriceTotal\",  # note: 'Total'\n",
    "        \"ag\": \"agPriceTotal\",  # note: 'Total'\n",
    "        \"pop\": \"popTotal\",\n",
    "    }\n",
    "\n",
    "    # verify fields exist; raise clear error otherwise\n",
    "    econ_fields_exist = {f.name for f in arcpy.ListFields(hazard_data[\"Economy\"])}\n",
    "    missing = [v for v in econ_fields.values() if v not in econ_fields_exist]\n",
    "    if missing:\n",
    "        raise RuntimeError(f\"Economy is missing fields: {missing}\")\n",
    "\n",
    "    targets = [econ_fields[k] for k in (\"ho\",\"ro\",\"fo\",\"ag\",\"pop\")]\n",
    "    sums = {k: 0.0 for k in (\"ho\",\"ro\",\"fo\",\"ag\",\"pop\")}\n",
    "\n",
    "    with arcpy.da.SearchCursor(hazard_data[\"Economy\"], targets) as cur:\n",
    "        for row in cur:\n",
    "            for k, val in zip((\"ho\",\"ro\",\"fo\",\"ag\",\"pop\"), row):\n",
    "                if val is None:\n",
    "                    continue\n",
    "                try:\n",
    "                    sums[k] += float(val)\n",
    "                except (TypeError, ValueError):\n",
    "                    try:\n",
    "                        sums[k] += float(str(val).replace(\" \", \"\").replace(\",\", \".\"))\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "    total_prices = sums\n",
    "    print(\"\\nTotal asset values (from Economy):\")\n",
    "    for k in (\"ho\",\"ro\",\"fo\",\"ag\",\"pop\"):\n",
    "        print(f\"  {k}: {total_prices[k]:,.0f}\")\n",
    "else:\n",
    "    print(\"\\n⚠️ Economy not loaded; totals skipped.\")\n",
    "\n",
    "econ_fc  = hazard_data[\"Economy\"]    \n",
    "def fc_to_df(fc_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Read non-geometry fields into a DataFrame (NULL-safe).\"\"\"\n",
    "    fld_objs = [f for f in arcpy.ListFields(fc_path) if f.type not in (\"Geometry\",\"Raster\")]\n",
    "    names = [f.name for f in fld_objs]\n",
    "    if not names:\n",
    "        return pd.DataFrame()\n",
    "    nulls = {}\n",
    "    for f in fld_objs:\n",
    "        t = f.type\n",
    "        nulls[f.name] = (-1 if t in (\"SmallInteger\",\"Integer\",\"OID\")\n",
    "                         else (np.nan if t in (\"Single\",\"Double\")\n",
    "                               else (\"\" if t in (\"String\",\"Guid\") else None)))\n",
    "    try:\n",
    "        arr = arcpy.da.TableToNumPyArray(econ_fc, names, skip_nulls=False, null_value=nulls)\n",
    "        eco = pd.DataFrame(arr)\n",
    "    except Exception:\n",
    "        rows = []\n",
    "        with arcpy.da.SearchCursor(econ_fc, names) as cur:\n",
    "            for rec in cur:\n",
    "                rows.append({n: (v if v is not None else nulls.get(n)) for n, v in zip(names, rec)})\n",
    "        eco = pd.DataFrame(rows)\n",
    "    # normalize strings\n",
    "    for c in eco.columns:\n",
    "        if eco[c].dtype.kind in (\"U\",\"S\",\"O\"):\n",
    "            eco[c] = eco[c].astype(object)\n",
    "    return eco\n",
    "\n",
    "def norm_key(s: pd.Series) -> pd.Series:\n",
    "    return s.astype(str).str.strip().str.lower()\n",
    "\n",
    "CAND_KEYS = (\"MO\", \"MO_NAME\")\n",
    "def pick_join_key(hgdf: pd.DataFrame, eco_df: pd.DataFrame) -> str:\n",
    "    for k in CAND_KEYS:\n",
    "        if k in hgdf.columns and k in eco_df.columns:\n",
    "            return k\n",
    "    common = [c for c in hgdf.columns if c in eco_df.columns and c.lower() != \"geometry\"]\n",
    "    if common:\n",
    "        return common[0]\n",
    "    raise ValueError(\"No common municipality key between hazard CSV and Economy.\")\n",
    "\n",
    "# Build the Economy DataFrame used by the donuts code\n",
    "eco = fc_to_df(econ_fc)\n",
    "\n",
    "# Economy field names (match your GDB schema)\n",
    "price_cols = {\n",
    "    \"ho\": \"hoPriceTot\",\n",
    "    \"ro\": \"roPriceTot\",\n",
    "    \"fo\": \"foPriceTotal\",   # note 'Total'\n",
    "    \"ag\": \"agPriceTotal\",   # note 'Total'\n",
    "    \"pop\": \"popTotal\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5debbb",
   "metadata": {},
   "source": [
    "Функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0721aa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_order = [\n",
    "    \"неопасно\",\n",
    "    \"умерено опасно\",\n",
    "    \"опасно\",\n",
    "    \"весьма опасно\",\n",
    "    \"очень опасно\"\n",
    "]\n",
    "asset_types = [\"ho\", \"ro\", \"fo\", \"ag\", \"pop\"]\n",
    "\n",
    "def summarize_by_hazard(hazard_name: str, fc_path: str, total_prices: dict) -> dict:\n",
    "    \"\"\"\n",
    "    fc_path = full path to a feature class in the GDB (e.g., D:\\...\\Default.gdb\\risks\\Yakutia_hazWildfire_...).\n",
    "    total_prices = dict with totals for ho/ro/fo/ag/pop from Economy.\n",
    "    Returns a single summary row (dict) for this hazard.\n",
    "    \"\"\"\n",
    "    row = {\"hazard\": hazard_name}\n",
    "\n",
    "    # --- 0) discover available fields once ---\n",
    "    fields_set = {f.name for f in arcpy.ListFields(fc_path)}\n",
    "\n",
    "    # --- 1) averages for % fields, sums for absolute fields ---\n",
    "    percent_cols = [\"rNF_ho\", \"rSF_ho\", \"rLF_ro\", \"rSF_ag\", \"rSF_fo\", \"rNF_pop\"]\n",
    "    abs_cols     = [\"rNiNA_ho\",\"rSiHA_ho\",\"rLiHA_ro\",\"rSiHA_ag\",\"rSiHA_fo\",\"rNiHA_pop\", 'rNaR_pop']\n",
    "\n",
    "    # Build a minimal field list for the cursor\n",
    "    needed_for_stats = [c for c in percent_cols + abs_cols if c in fields_set]\n",
    "\n",
    "    # --- 2) category column & damage columns by asset ---\n",
    "    cat_col = \"hNormTot\" if hazard_name == \"Flood\" else \"hExpertTot\"\n",
    "    has_cat = cat_col in fields_set\n",
    "\n",
    "    dmg_cols_map = {}\n",
    "    for a in asset_types:\n",
    "        if hazard_name == \"Wildfire\":\n",
    "            dmg_cols_map[a] = f\"rNaR_{a}\" if a == \"pop\" else f\"rVaR_{a}\"\n",
    "        else:\n",
    "            dmg_cols_map[a] = f\"rNiHA_{a}\" if a == \"pop\" else f\"rViHA_{a}\"\n",
    "\n",
    "    # Which of those actually exist?\n",
    "    dmg_cols_present = {a: c for a, c in dmg_cols_map.items() if c in fields_set}\n",
    "\n",
    "    # Final cursor field list: stats + (cat_col if present) + all present dmg cols\n",
    "    cursor_fields = []\n",
    "    cursor_fields.extend(needed_for_stats)\n",
    "    if has_cat:\n",
    "        cursor_fields.append(cat_col)\n",
    "    cursor_fields.extend(sorted(set(dmg_cols_present.values())))  # unique\n",
    "\n",
    "    if not cursor_fields:\n",
    "        # still emit shape for table: zeros and pct/value columns\n",
    "        for c in percent_cols:\n",
    "            row[f\"{c}_pct\"] = 0.0\n",
    "        for c in abs_cols:\n",
    "            row[c] = 0.0\n",
    "        for a in asset_types:\n",
    "            dmg_col = dmg_cols_map[a]\n",
    "            row[dmg_col] = 0.0\n",
    "            for i, _cat in enumerate(category_order):\n",
    "                row[f\"{dmg_col}_pct_{i}\"] = 0.0\n",
    "                row[f\"{dmg_col}_val_{i}\"] = 0.0\n",
    "        return row\n",
    "\n",
    "\n",
    "    # --- accumulators ---\n",
    "    pct_sums = {c: 0.0 for c in percent_cols if c in fields_set}\n",
    "    pct_counts = {c: 0 for c in percent_cols if c in fields_set}\n",
    "    abs_sums = {c: 0.0 for c in abs_cols if c in fields_set}\n",
    "\n",
    "    # per-asset, per-category sums\n",
    "    by_asset_by_cat = {\n",
    "        a: {cat: 0.0 for cat in category_order} for a in asset_types\n",
    "    }\n",
    "\n",
    "    # index mapping for fast access\n",
    "    idx = {name: i for i, name in enumerate(cursor_fields)}\n",
    "    # Iterate rows once\n",
    "    with arcpy.da.SearchCursor(fc_path, cursor_fields) as cur:\n",
    "        for rec in cur:\n",
    "            # 1) percent means\n",
    "            for c in pct_sums.keys():\n",
    "                v = rec[idx[c]]\n",
    "                if v is not None:\n",
    "                    try:\n",
    "                        fv = float(v)\n",
    "                        if isfinite(fv):\n",
    "                            pct_sums[c] += fv\n",
    "                            pct_counts[c] += 1\n",
    "                    except Exception:\n",
    "                        pass\n",
    "\n",
    "            # 2) abs sums\n",
    "            for c in abs_sums.keys():\n",
    "                v = rec[idx[c]]\n",
    "                if v is not None:\n",
    "                    try:\n",
    "                        abs_sums[c] += float(v)\n",
    "                    except Exception:\n",
    "                        # try forgiving parse\n",
    "                        try:\n",
    "                            abs_sums[c] += float(str(v).replace(\" \", \"\").replace(\",\", \".\"))\n",
    "                        except Exception:\n",
    "                            pass\n",
    "\n",
    "            # 3) category splits (only if we have a category col)\n",
    "            if has_cat:\n",
    "                cat_val = rec[idx[cat_col]]\n",
    "                # keep as-is; if None or not one of the known categories, treat as \"неопасно\"\n",
    "                cat_key = cat_val if cat_val in category_order else \"неопасно\"\n",
    "\n",
    "                for a, dmg_col in dmg_cols_present.items():\n",
    "                    v = rec[idx[dmg_col]]\n",
    "                    if v is None:\n",
    "                        continue\n",
    "                    try:\n",
    "                        add = float(v)\n",
    "                    except Exception:\n",
    "                        try:\n",
    "                            add = float(str(v).replace(\" \", \"\").replace(\",\", \".\"))\n",
    "                        except Exception:\n",
    "                            add = 0.0\n",
    "                    by_asset_by_cat[a][cat_key] += add\n",
    "\n",
    "    # write percent means (%)\n",
    "    for c in percent_cols:\n",
    "        if c in pct_sums:\n",
    "            n = pct_counts.get(c, 0)\n",
    "            mean_val = (pct_sums[c] / n) if n > 0 else 0.0\n",
    "            row[f\"{c}_pct\"] = float(mean_val * 100.0)  # your original *100\n",
    "        else:\n",
    "            row[f\"{c}_pct\"] = 0.0\n",
    "\n",
    "    # write abs sums\n",
    "    for c in abs_cols:\n",
    "        row[c] = float(abs_sums.get(c, 0.0))\n",
    "\n",
    "    # 4) per-asset totals + category shares + absolute values\n",
    "    # 4) per-asset totals + category shares + absolute values\n",
    "    danger_cats = category_order[1:]  # [\"умерено опасно\",\"опасно\",\"весьма опасно\",\"очень опасно\"]\n",
    "\n",
    "    for a in asset_types:\n",
    "        dmg_col = dmg_cols_map[a]\n",
    "\n",
    "        # absolute affected amounts per hazard category (aggregated over rows)\n",
    "        sums_by_cat = by_asset_by_cat.get(a, {cat: 0.0 for cat in category_order})\n",
    "\n",
    "        # total affected (across ALL cats as read from the FC; for \"неопасно\" it should be 0 anyway)\n",
    "        affected_total = float(sum(sums_by_cat.get(cat, 0.0) for cat in danger_cats))\n",
    "\n",
    "        # keep the total affected for this asset (same meaning as before)\n",
    "        row[dmg_col] = affected_total\n",
    "\n",
    "        # Economy denominator for the whole region\n",
    "        tot = float(total_prices.get(a, 0.0))\n",
    "\n",
    "        # --- values per category ---\n",
    "        # cats 1..4: already have affected values\n",
    "        # cat 0 (\"неопасно\"): residual = total - affected_total (clip at 0)\n",
    "        safe_val = max(tot - affected_total, 0.0) if tot > 0 else 0.0\n",
    "\n",
    "        # write absolute values\n",
    "        for i, cat in enumerate(category_order):\n",
    "            if i == 0:\n",
    "                row[f\"{dmg_col}_val_{i}\"] = safe_val\n",
    "            else:\n",
    "                row[f\"{dmg_col}_val_{i}\"] = float(sums_by_cat.get(cat, 0.0))\n",
    "\n",
    "        # --- percent shares per category ---\n",
    "        if tot > 0:\n",
    "            # cats 1..4 from affected / tot\n",
    "            shares = {cat: (sums_by_cat.get(cat, 0.0) / tot) * 100.0 for cat in danger_cats}\n",
    "            # cat 0 from residual to close to 100%\n",
    "            shares_others_sum = sum(shares.values())\n",
    "            shares_neopasno = max(100.0 - shares_others_sum, 0.0)\n",
    "        else:\n",
    "            shares = {cat: 0.0 for cat in danger_cats}\n",
    "            shares_neopasno = 0.0\n",
    "\n",
    "        # write percents in fixed 0..4 order\n",
    "        for i, cat in enumerate(category_order):\n",
    "            if i == 0:\n",
    "                row[f\"{dmg_col}_pct_{i}\"] = float(shares_neopasno)\n",
    "            else:\n",
    "                row[f\"{dmg_col}_pct_{i}\"] = float(shares.get(cat, 0.0))\n",
    "\n",
    "\n",
    "    return row\n",
    "\n",
    "# ---------- formatting helpers (unchanged from your spec) ----------\n",
    "def _round_sig(x: float, sig: int = 3) -> float:\n",
    "    if not isfinite(x) or x == 0:\n",
    "        return 0.0\n",
    "    return round(x, sig - 1 - floor(log10(abs(x))))\n",
    "\n",
    "def _fmt_clean(x: float) -> str:\n",
    "    s = f\"{x:.10f}\".rstrip('0').rstrip('.')\n",
    "    return s if s else \"0\"\n",
    "\n",
    "def format_center(asset: str, value: float) -> str:\n",
    "    if not np.isfinite(value):\n",
    "        return \"\"\n",
    "    if value == 0:\n",
    "        return \"0\"\n",
    "\n",
    "    unit = \"чел.\" if asset == \"pop\" else \"₽\"\n",
    "    abs_v = abs(value)\n",
    "    if abs_v >= 1_000_000_000_000:\n",
    "        scaled = value / 1_000_000_000_000.0\n",
    "        return f\"{_fmt_clean(_round_sig(scaled, 3))} трлн. {unit}\"\n",
    "    elif abs_v >= 1_000_000_000:\n",
    "        scaled = value / 1_000_000_000.0\n",
    "        return f\"{_fmt_clean(_round_sig(scaled, 3))} млрд. {unit}\"\n",
    "    elif abs_v >= 1_000_000:\n",
    "        scaled = value / 1_000_000.0\n",
    "        return f\"{_fmt_clean(_round_sig(scaled, 3))} млн. {unit}\"\n",
    "    else:\n",
    "        scaled = value / 1_000.0\n",
    "        return f\"{_fmt_clean(_round_sig(scaled, 3))} тыс. {unit}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbf97c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing Heat …\n",
      "Summarizing Freeze …\n",
      "Summarizing Drought …\n",
      "Summarizing Thunder …\n",
      "Summarizing Hail …\n",
      "Summarizing Precipitation …\n",
      "Summarizing Wind …\n",
      "Summarizing Cold …\n",
      "Summarizing Flood …\n",
      "Summarizing Permafrost …\n",
      "Summarizing Wildfire …\n",
      "✅ Saved summary outside GDB:\n",
      "C:\\Users\\sdfsg\\Downloads\\Yakutia_RiskProfile_10-10-2025\\Yakutia_RiskProfile_08-10-2025\\Yakutia_RiskProfile\\hazard_summary_by_risk.csv\n"
     ]
    }
   ],
   "source": [
    "# === Build the table ===\n",
    "rows = []\n",
    "for hz, fc_path in hazard_data.items():\n",
    "    if hz == \"Economy\":\n",
    "        continue\n",
    "    print(f\"Summarizing {hz} …\")\n",
    "    \n",
    "    row = summarize_by_hazard(hz, fc_path, total_prices)\n",
    "    row[\"region\"] = region\n",
    "    rows.append(row)\n",
    "\n",
    "\n",
    "summary = pd.DataFrame(rows)\n",
    "\n",
    "# round all percentage columns to 1 decimal\n",
    "for col in summary.columns:\n",
    "    if \"_pct\" in col:\n",
    "        summary[col] = summary[col].round(1)\n",
    "\n",
    "# === Save CSV next to the GDB (not inside it) ===\n",
    "out_dir = os.path.dirname(gdb_path)  # one level above the .gdb\n",
    "out_csv = os.path.join(out_dir, \"hazard_summary_by_risk.csv\")\n",
    "\n",
    "# Save with UTF-8 encoding (handles Cyrillic safely)\n",
    "summary.to_csv(out_csv, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"✅ Saved summary outside GDB:\\n{out_csv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb7da51",
   "metadata": {},
   "source": [
    "Далее код, создающий колонки nodata = 100 для отсутствующих значений процентов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bc25b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: Heat -> C:\\Users\\sdfsg\\Downloads\\Yakutia_RiskProfile_10-10-2025\\Yakutia_RiskProfile_08-10-2025\\Yakutia_RiskProfile\\out_with_pct_Yakutia_nodata\\Heat_withPct.csv\n",
      "✅ Saved: Freeze -> C:\\Users\\sdfsg\\Downloads\\Yakutia_RiskProfile_10-10-2025\\Yakutia_RiskProfile_08-10-2025\\Yakutia_RiskProfile\\out_with_pct_Yakutia_nodata\\Freeze_withPct.csv\n",
      "✅ Saved: Drought -> C:\\Users\\sdfsg\\Downloads\\Yakutia_RiskProfile_10-10-2025\\Yakutia_RiskProfile_08-10-2025\\Yakutia_RiskProfile\\out_with_pct_Yakutia_nodata\\Drought_withPct.csv\n",
      "✅ Saved: Thunder -> C:\\Users\\sdfsg\\Downloads\\Yakutia_RiskProfile_10-10-2025\\Yakutia_RiskProfile_08-10-2025\\Yakutia_RiskProfile\\out_with_pct_Yakutia_nodata\\Thunder_withPct.csv\n",
      "✅ Saved: Hail -> C:\\Users\\sdfsg\\Downloads\\Yakutia_RiskProfile_10-10-2025\\Yakutia_RiskProfile_08-10-2025\\Yakutia_RiskProfile\\out_with_pct_Yakutia_nodata\\Hail_withPct.csv\n",
      "✅ Saved: Precipitation -> C:\\Users\\sdfsg\\Downloads\\Yakutia_RiskProfile_10-10-2025\\Yakutia_RiskProfile_08-10-2025\\Yakutia_RiskProfile\\out_with_pct_Yakutia_nodata\\Precipitation_withPct.csv\n",
      "✅ Saved: Wind -> C:\\Users\\sdfsg\\Downloads\\Yakutia_RiskProfile_10-10-2025\\Yakutia_RiskProfile_08-10-2025\\Yakutia_RiskProfile\\out_with_pct_Yakutia_nodata\\Wind_withPct.csv\n",
      "✅ Saved: Cold -> C:\\Users\\sdfsg\\Downloads\\Yakutia_RiskProfile_10-10-2025\\Yakutia_RiskProfile_08-10-2025\\Yakutia_RiskProfile\\out_with_pct_Yakutia_nodata\\Cold_withPct.csv\n",
      "✅ Saved: Flood -> C:\\Users\\sdfsg\\Downloads\\Yakutia_RiskProfile_10-10-2025\\Yakutia_RiskProfile_08-10-2025\\Yakutia_RiskProfile\\out_with_pct_Yakutia_nodata\\Flood_withPct.csv\n",
      "✅ Saved: Permafrost -> C:\\Users\\sdfsg\\Downloads\\Yakutia_RiskProfile_10-10-2025\\Yakutia_RiskProfile_08-10-2025\\Yakutia_RiskProfile\\out_with_pct_Yakutia_nodata\\Permafrost_withPct.csv\n",
      "✅ Saved: Wildfire -> C:\\Users\\sdfsg\\Downloads\\Yakutia_RiskProfile_10-10-2025\\Yakutia_RiskProfile_08-10-2025\\Yakutia_RiskProfile\\out_with_pct_Yakutia_nodata\\Wildfire_withPct.csv\n"
     ]
    }
   ],
   "source": [
    "out_dir = Path(gdb_path).parent / \"out_with_pct_Yakutia_nodata\"\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- config ---\n",
    "category_order = [\"неопасно\",\"умерено опасно\",\"опасно\",\"весьма опасно\",\"очень опасно\"]\n",
    "cat_to_idx = {name: i for i, name in enumerate(category_order)}\n",
    "asset_types = [\"ho\",\"ro\",\"fo\",\"ag\",\"pop\"]\n",
    "econ_fc = hazard_data[\"Economy\"]\n",
    "\n",
    "price_cols = {\n",
    "    \"ho\": \"hoPriceTot\",\n",
    "    \"ro\": \"roPriceTot\",\n",
    "    \"fo\": \"foPriceTotal\",   # note: Total\n",
    "    \"ag\": \"agPriceTotal\",   # note: Total\n",
    "    \"pop\": \"popTotal\",\n",
    "}\n",
    "percent_cols = [\"rNF_ho\",\"rSF_ho\",\"rLF_ro\",\"rSF_ag\",\"rSF_fo\",\"rNF_pop\"]\n",
    "CAND_KEYS = [\"MO_NAME\",\"MO\"]\n",
    "\n",
    "def asset_cols(asset: str):\n",
    "    return [f\"{asset}_pct_{i}\" for i in range(5)] + [f\"{asset}_pct_nodata\"]\n",
    "\n",
    "def asset_val_cols(asset: str):\n",
    "    return [f\"{asset}_val_{i}\" for i in range(5)]\n",
    "\n",
    "def pick_join_key(hdf: pd.DataFrame, econ_fc: pd.DataFrame) -> str:\n",
    "    for k in CAND_KEYS:\n",
    "        if k in hdf.columns and k in econ_fc.columns:\n",
    "            return k\n",
    "    common = [c for c in hdf.columns if c in econ_fc.columns and c.lower() != \"geometry\"]\n",
    "    if common:\n",
    "        return common[0]\n",
    "    raise ValueError(\"No common municipality key found between hazard and Economy.\")\n",
    "\n",
    "def norm_key(s: pd.Series) -> pd.Series:\n",
    "    return s.astype(str).str.strip().str.lower()\n",
    "\n",
    "def fc_to_df(fc_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read all non-geometry fields; handle NULLs so TableToNumPyArray doesn't fail on ints.\n",
    "    Falls back to SearchCursor if needed.\n",
    "    \"\"\"\n",
    "    fld_objs = [f for f in arcpy.ListFields(fc_path) if f.type not in (\"Geometry\",\"Raster\")]\n",
    "    fields = [f.name for f in fld_objs]\n",
    "    if not fields:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    null_value = {}\n",
    "    for f in fld_objs:\n",
    "        t = f.type\n",
    "        if t in (\"SmallInteger\",\"Integer\",\"OID\"):\n",
    "            null_value[f.name] = -1\n",
    "        elif t in (\"Single\",\"Double\"):\n",
    "            null_value[f.name] = np.nan\n",
    "        elif t in (\"String\",\"Guid\"):\n",
    "            null_value[f.name] = \"\"\n",
    "        elif t == \"Date\":\n",
    "            null_value[f.name] = None\n",
    "        else:\n",
    "            null_value[f.name] = \"\"\n",
    "\n",
    "    try:\n",
    "        arr = arcpy.da.TableToNumPyArray(\n",
    "            in_table=fc_path,\n",
    "            field_names=fields,\n",
    "            skip_nulls=False,\n",
    "            null_value=null_value\n",
    "        )\n",
    "        df = pd.DataFrame(arr)\n",
    "    except Exception:\n",
    "        # Fallback: SearchCursor row-by-row with the same null defaults\n",
    "        rows = []\n",
    "        with arcpy.da.SearchCursor(fc_path, fields) as cur:\n",
    "            for rec in cur:\n",
    "                d = {}\n",
    "                for name, val in zip(fields, rec):\n",
    "                    if val is None:\n",
    "                        d[name] = null_value.get(name, None)\n",
    "                    else:\n",
    "                        d[name] = val\n",
    "                rows.append(d)\n",
    "        df = pd.DataFrame(rows)\n",
    "\n",
    "    # Normalize string dtypes\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype.kind in (\"U\",\"S\",\"O\"):\n",
    "            df[c] = df[c].astype(object)\n",
    "    return df\n",
    "\n",
    "# --- Economy DF (for joins + region + denominators) ---\n",
    "eco_fc = hazard_data[\"Economy\"]\n",
    "eco_df = fc_to_df(eco_fc)\n",
    "region_name = (\n",
    "    eco_df[\"region\"].dropna().astype(str).unique()[0]\n",
    "    if \"region\" in eco_df.columns and eco_df[\"region\"].notna().any()\n",
    "    else \"Unknown\"\n",
    ")\n",
    "\n",
    "# ============ PER-HAZARD MUNICIPALITY TABLES (no combined file) ============\n",
    "for hazard_name, fc_path in hazard_data.items():\n",
    "    if hazard_name == \"Economy\":\n",
    "        continue\n",
    "\n",
    "    gdf = fc_to_df(fc_path)\n",
    "\n",
    "    # Scale % columns idempotently\n",
    "    for c in percent_cols:\n",
    "        if c in gdf.columns:\n",
    "            s = pd.to_numeric(gdf[c], errors=\"coerce\")\n",
    "            finite = s[np.isfinite(s)]\n",
    "            if not finite.empty and (finite <= 1.0).mean() >= 0.90:\n",
    "                gdf[c] = (s * 100.0).round(1)\n",
    "            else:\n",
    "                gdf[c] = s.round(1)\n",
    "\n",
    "    key = pick_join_key(gdf, eco_df)\n",
    "\n",
    "    gdf = gdf.copy()\n",
    "    eco_tmp = eco_df[[k for k in [key,\"region\"] if k in eco_df.columns] + list(price_cols.values())].copy()\n",
    "\n",
    "    gdf[\"_join_key\"] = norm_key(gdf[key])\n",
    "    eco_tmp[\"_join_key\"] = norm_key(eco_tmp[key])\n",
    "    eco_idx = eco_tmp.set_index(\"_join_key\")\n",
    "\n",
    "    # Municipality totals per asset\n",
    "    mo_totals = {\n",
    "        a: (eco_idx[price_cols[a]] if price_cols[a] in eco_idx.columns else pd.Series(dtype=float))\n",
    "        for a in asset_types\n",
    "    }\n",
    "\n",
    "    # Region\n",
    "    if \"region\" in eco_idx.columns:\n",
    "        gdf[\"region\"] = gdf[\"_join_key\"].map(eco_idx[\"region\"]).fillna(region_name)\n",
    "    else:\n",
    "        gdf[\"region\"] = region_name\n",
    "\n",
    "    # Category column\n",
    "    cat_raw = gdf.get(\"hNormLive\") if hazard_name == \"Flood\" else gdf.get(\"hExpertTot\")\n",
    "    cat_idx = pd.Series(0, index=gdf.index, dtype=int)\n",
    "    if cat_raw is not None:\n",
    "        cat_to_idx_lower = {k.lower(): v for k, v in cat_to_idx.items()}\n",
    "        cat_idx = cat_raw.astype(str).str.lower().map(cat_to_idx_lower).fillna(0).astype(int)\n",
    "    gdf[\"_cat_idx\"] = cat_idx.clip(0, 4)\n",
    "\n",
    "    # Initialize share (pct) and value columns\n",
    "    for a in asset_types:\n",
    "        for col in asset_cols(a):\n",
    "            gdf[col] = 0.0\n",
    "        for col in asset_val_cols(a):\n",
    "            gdf[col] = 0.0\n",
    "\n",
    "    for a in asset_types:\n",
    "        if hazard_name == \"Wildfire\":\n",
    "            dmg_col = f\"rNaR_{a}\" if a == \"pop\" else f\"rVaR_{a}\"\n",
    "        else:\n",
    "            dmg_col = f\"rNiHA_{a}\" if a == \"pop\" else f\"rViHA_{a}\"\n",
    "\n",
    "        denom = gdf[\"_join_key\"].map(mo_totals[a])\n",
    "        has_assets = denom.fillna(0) > 0\n",
    "\n",
    "        exists_col = f\"{a}_exists\"\n",
    "        gdf[exists_col] = has_assets.astype(bool)\n",
    "\n",
    "        tok = re.compile(rf\"(^|_){re.escape(a)}(_|$)\")\n",
    "        candidate_cols = [c for c in gdf.columns if tok.search(c)]\n",
    "        wipe_cols = [c for c in candidate_cols if not c.endswith(\"_exists\")]\n",
    "\n",
    "        mask_no_assets = ~has_assets\n",
    "        for col in wipe_cols:\n",
    "            if pd.api.types.is_bool_dtype(gdf[col].dtype):\n",
    "                gdf.loc[mask_no_assets, col] = False\n",
    "            else:\n",
    "                gdf.loc[mask_no_assets, col] = np.nan\n",
    "\n",
    "        # absolute affected value for this asset in this MO\n",
    "        if dmg_col in gdf.columns:\n",
    "            num = pd.to_numeric(gdf[dmg_col], errors=\"coerce\")\n",
    "        else:\n",
    "            num = pd.Series(np.nan, index=gdf.index)\n",
    "\n",
    "        # % affected of municipal total\n",
    "        pct = pd.Series(np.nan, index=gdf.index)\n",
    "        valid_mask = has_assets & (~pd.isna(num)) & (denom > 0)\n",
    "        pct.loc[valid_mask] = (num.loc[valid_mask] / denom.loc[valid_mask]) * 100.0\n",
    "        pct = pct.clip(lower=0.0, upper=100.0).round(1)\n",
    "\n",
    "        # % and value of the SAFE part\n",
    "        pct_safe = (100.0 - pct).clip(lower=0.0, upper=100.0).round(1)\n",
    "        val_safe = (denom - num).where(valid_mask, np.nan)\n",
    "        val_safe = val_safe.mask(val_safe < 0, 0.0)  # clip negatives to 0\n",
    "\n",
    "        pct_cols = [f\"{a}_pct_{i}\" for i in range(5)]  # без nodata\n",
    "        val_cols = [f\"{a}_val_{i}\" for i in range(5)]\n",
    "\n",
    "        # write category-specific cells\n",
    "        for i in range(5):\n",
    "            mask_cat = (gdf[\"_cat_idx\"] == i) & has_assets\n",
    "            if i == 0:\n",
    "                # \"неопасно\" column gets the SAFE share/value\n",
    "                gdf.loc[mask_cat, pct_cols[0]] = pct_safe.loc[mask_cat]\n",
    "                gdf.loc[mask_cat, val_cols[0]] = val_safe.loc[mask_cat]\n",
    "            else:\n",
    "                # dangerous category gets affected share/value\n",
    "                gdf.loc[mask_cat, pct_cols[i]] = pct.loc[mask_cat]\n",
    "                gdf.loc[mask_cat, val_cols[i]] = num.loc[mask_cat]\n",
    "\n",
    "        # No extra residual math needed now; we've set pct_0 directly.\n",
    "\n",
    "        # residual \n",
    "        residual = (100.0 - pct).clip(lower=0.0, upper=100.0)\n",
    "\n",
    "        # --- absolute residual goes to val_0 ---\n",
    "        # denom = total municipal asset value; num = affected/at-risk absolute value\n",
    "        residual_abs = (denom - num)\n",
    "        residual_abs = residual_abs.where(residual_abs > 0, 0.0)  # clip at 0\n",
    "\n",
    "        mask_cat0  = (gdf[\"_cat_idx\"] == 0) & has_assets\n",
    "        mask_non0  = (gdf[\"_cat_idx\"] != 0) & has_assets\n",
    "\n",
    "        # If municipality is \"неопасно\": all value is non-dangerous → val_0 = total\n",
    "        gdf.loc[mask_cat0, val_cols[0]] = denom.loc[mask_cat0]\n",
    "\n",
    "        # If municipality is in category 1..4: split into dangerous vs safe\n",
    "        #   - val_{cat} = num (already set above)\n",
    "        #   - val_0     = denom - num (unaffected)\n",
    "        gdf.loc[mask_non0, val_cols[0]] = residual_abs.loc[mask_non0]\n",
    "\n",
    "        # keep your existing % residual behavior (affects only pct_0)\n",
    "        mask_non0_pct = (gdf[\"_cat_idx\"] != 0) & (residual > 0) & has_assets\n",
    "        gdf.loc[mask_non0_pct, pct_cols[0]] += residual.loc[mask_non0_pct]\n",
    "\n",
    "        # ===== ИСПРАВЛЕННЫЙ КОД: Правильная логика для nodata =====\n",
    "        # Список всех pct колонок (включая pct_0!)\n",
    "        all_pct_cols = [f\"{a}_pct_{i}\" for i in range(5)]\n",
    "        \n",
    "        # Заменяем только NULL/NaN на 0 в pct колонках, но не перезаписываем существующие значения\n",
    "        for col in all_pct_cols:\n",
    "            if col in gdf.columns:\n",
    "                # Заменяем только NaN/None на 0, сохраняя существующие значения\n",
    "                gdf[col] = gdf[col].fillna(0.0)\n",
    "        \n",
    "        # Заменяем NULL в val колонках\n",
    "        for col in val_cols:\n",
    "            if col in gdf.columns:\n",
    "                gdf[col] = gdf[col].fillna(0.0)\n",
    "        \n",
    "        # Проверяем что ВСЕ pct колонки (0-4) равны 0\n",
    "        all_pct_zero_mask = True\n",
    "        for col in all_pct_cols:\n",
    "            if col in gdf.columns:\n",
    "                all_pct_zero_mask = all_pct_zero_mask & (gdf[col] == 0)\n",
    "        \n",
    "        # Устанавливаем значения для *_pct_nodata\n",
    "        nodata_col = f\"{a}_pct_nodata\"\n",
    "        \n",
    "        # Для строк где есть активы\n",
    "        if has_assets.any():\n",
    "            # Если ВСЕ pct колонки (0-4) = 0, то nodata = 100, иначе 0\n",
    "            gdf.loc[has_assets & all_pct_zero_mask, nodata_col] = 100.0\n",
    "            gdf.loc[has_assets & (~all_pct_zero_mask), nodata_col] = 0.0\n",
    "        else:\n",
    "            # Если активов нет, устанавливаем nodata в 0\n",
    "            gdf.loc[:, nodata_col] = 0.0\n",
    "\n",
    "                    # ===== ДОПОЛНИТЕЛЬНО: Гарантируем что в nodata нет NULL =====\n",
    "        # Заменяем все оставшиеся NULL в nodata колонке\n",
    "        gdf[nodata_col] = gdf[nodata_col].fillna(100)\n",
    "\n",
    "    # Cleanup\n",
    "    drop_cols = [\n",
    "        \"_join_key\",\"_cat_idx\",\"Shape_Length\",\"Shape_Area\",\"Shape_Leng\",\"geometry\",\n",
    "        \"area\",\"areaLive\",\"areaLiveF\",\"sTot\",\"sTotFrac\",\"sLive\",\"sLiveFrac\",\"sDensity\",\n",
    "        \"spTot\",\"spTotFrac\",\"spLive\",\"spLiveFrac\",\"ieMeanTot\",\"ieMaxTot\",\"ieMeanLiv\",\"ieMaxLiv\",\n",
    "        \"itMeanTot\",\"itMeanLive\",\"iOverTot\",\"iOverLiv\",\"fdMeanTot\",\"fdMaxTot\",\"fdMeanLive\",\"fdMaxLive\",\n",
    "        \"feMeanTot\",\"feMeanLive\",\"hFxSTot\",\"hFxSLive\",\"Риски\",\"Риск_1\",\n",
    "    ]\n",
    "    gdf = gdf.drop(columns=drop_cols, errors=\"ignore\")\n",
    "    gdf[\"hazard\"] = hazard_name\n",
    "\n",
    "    # Cast numeric; round pct columns\n",
    "    for c in gdf.columns:\n",
    "        if pd.api.types.is_bool_dtype(gdf[c].dtype):\n",
    "            continue\n",
    "        if pd.api.types.is_integer_dtype(gdf[c]) or pd.api.types.is_float_dtype(gdf[c]):\n",
    "            gdf[c] = pd.to_numeric(gdf[c], errors=\"coerce\").astype(\"float64\")\n",
    "    pct_like = [c for c in gdf.columns if \"_pct\" in c]\n",
    "    if pct_like:\n",
    "        gdf[pct_like] = gdf[pct_like].round(1)\n",
    "\n",
    "    # Save per-hazard CSV (outside gdb)\n",
    "    out_path = out_dir / f\"{hazard_name}_withPct.csv\"\n",
    "    gdf.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"✅ Saved: {hazard_name} -> {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b1d8e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib\n",
    "# Force a headless backend so it never crashes on GUI issues\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Cyrillic-safe font (for labels like \"неопасно\")\n",
    "plt.rcParams[\"font.family\"] = \"DejaVu Sans\"\n",
    "plt.rcParams[\"svg.fonttype\"] = \"none\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c8c1ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-1425890ad5b7>:94: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "base_dir = Path(gdb_path).parent\n",
    "summary = pd.read_csv(base_dir / \"hazard_summary_by_risk.csv\", encoding=\"utf-8-sig\")\n",
    "# === Settings ===\n",
    "asset_types = [\"ho\",\"ro\",\"fo\",\"ag\",\"pop\"]\n",
    "category_order = [\"неопасно\", \"умерено опасно\", \"опасно\", \"весьма опасно\", \"очень опасно\"]\n",
    "category_colors = {\n",
    "    \"неопасно\": \"#a5de94\",\n",
    "    \"умерено опасно\": \"#feea80\",\n",
    "    \"опасно\": \"#fec979\",\n",
    "    \"весьма опасно\": \"#fb8066\",\n",
    "    \"очень опасно\": \"#f65356\",\n",
    "}\n",
    "plt.rcParams['svg.fonttype'] = 'none'\n",
    "\n",
    "def _base_prefix(hazard: str, asset: str) -> str:\n",
    "    \"\"\"Return rViHA / rNiHA / rVaR depending on hazard & asset.\"\"\"\n",
    "    if str(hazard) == \"Wildfire\":\n",
    "        if asset != \"pop\":\n",
    "            return \"rVaR\"\n",
    "        else: return 'rNaR'\n",
    "    return \"rNiHA\" if asset == \"pop\" else \"rViHA\"\n",
    "\n",
    "def share_cols(hazard: str, asset: str) -> list[str]:\n",
    "    \"\"\"rXX_{asset}_pct_0..4 in the order of category_order.\"\"\"\n",
    "    base = _base_prefix(hazard, asset)\n",
    "    return [f\"{base}_{asset}_pct_{i}\" for i in range(len(category_order))]\n",
    "\n",
    "def total_key(hazard: str, asset: str) -> str:\n",
    "    \"\"\"rXX_{asset} total.\"\"\"\n",
    "    base = _base_prefix(hazard, asset)\n",
    "    return f\"{base}_{asset}\"\n",
    "\n",
    "def autopct_fmt(p: float) -> str:\n",
    "    if p <= 0:\n",
    "        return ''\n",
    "    elif p >= 1:\n",
    "        return f\"{p:.0f}%\"\n",
    "    elif p >= 0.1:\n",
    "        return f\"{p:.1f}%\"\n",
    "    else:\n",
    "        return f\"{p:.2f}%\"\n",
    "\n",
    "num_hazards = len(summary)\n",
    "fig, axes = plt.subplots(num_hazards, len(asset_types), figsize=(22, 5 * max(1, num_hazards)))\n",
    "\n",
    "# normalize axes to 2D\n",
    "if num_hazards == 1 and len(asset_types) == 1:\n",
    "    axes = np.array([[axes]])\n",
    "elif num_hazards == 1:\n",
    "    axes = np.array(axes)[np.newaxis, :]\n",
    "elif len(asset_types) == 1:\n",
    "    axes = np.array(axes)[:, np.newaxis]\n",
    "\n",
    "for row_idx, (_, hz_row) in enumerate(summary.iterrows()):\n",
    "    hazard_name = hz_row.get(\"hazard\", f\"Hazard {row_idx+1}\")\n",
    "\n",
    "    for col_idx, a in enumerate(asset_types):\n",
    "        ax = axes[row_idx, col_idx]\n",
    "\n",
    "        # shares\n",
    "        cols = share_cols(hazard_name, a)\n",
    "        shares = [float(hz_row.get(c, 0) or 0) for c in cols]\n",
    "        colors = [category_colors[c] for c in category_order]\n",
    "\n",
    "        if sum(shares) <= 0:\n",
    "            ax.pie([1.0], labels=None, colors=[\"#eeeeee\"], startangle=90,\n",
    "                   counterclock=True, wedgeprops=dict(width=0.5))\n",
    "        else:\n",
    "            ax.pie(\n",
    "            shares,\n",
    "            labels=None,\n",
    "            autopct=autopct_fmt,\n",
    "            colors=colors,\n",
    "            startangle=90,\n",
    "            counterclock=True,\n",
    "            wedgeprops=dict(width=0.5),\n",
    "            pctdistance=1.2,\n",
    "            textprops=dict(fontsize=16),\n",
    "        )\n",
    "\n",
    "        # center label = global total (from Economy)\n",
    "        ax.text(0, 0, format_center(a, total_prices[a]),\n",
    "                ha='center', va='center',\n",
    "                fontsize=16, weight='bold')\n",
    "\n",
    "        ax.set_title(f\"{hz_row.get('hazard', f'Hazard {row_idx+1}')} — {a}\", fontsize=28)\n",
    "\n",
    "# Legend & save\n",
    "# fig.legend(category_order, loc='center left', bbox_to_anchor=(0.98, 0.5), title=\"Категории\")\n",
    "plt.tight_layout(rect=[0, 0, 0.95, 1])\n",
    "out_dir = Path(f\"{base_dir}/out_with_pct\"); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "plt.savefig(out_dir / \"hazard_donuts_all_with_pop.svg\", bbox_inches='tight')\n",
    "plt.savefig(out_dir / \"hazard_donuts_all_with_pop.png\", bbox_inches='tight', dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c10a036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-f32c18669163>:181: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sdfsg\\Documents\\work\\Perm_RiskProfile2Heat_donuts_by_municipality_wide.svg C:\\Users\\sdfsg\\Documents\\work\\Perm_RiskProfile2Heat_donuts_by_municipality_wide.png\n",
      "Cold\n",
      "C:\\Users\\sdfsg\\Documents\\work\\Perm_RiskProfile2Cold_donuts_by_municipality_wide.svg C:\\Users\\sdfsg\\Documents\\work\\Perm_RiskProfile2Cold_donuts_by_municipality_wide.png\n",
      "Precipitation\n",
      "C:\\Users\\sdfsg\\Documents\\work\\Perm_RiskProfile2Precipitation_donuts_by_municipality_wide.svg C:\\Users\\sdfsg\\Documents\\work\\Perm_RiskProfile2Precipitation_donuts_by_municipality_wide.png\n",
      "Wind\n",
      "C:\\Users\\sdfsg\\Documents\\work\\Perm_RiskProfile2Wind_donuts_by_municipality_wide.svg C:\\Users\\sdfsg\\Documents\\work\\Perm_RiskProfile2Wind_donuts_by_municipality_wide.png\n",
      "Freeze\n",
      "C:\\Users\\sdfsg\\Documents\\work\\Perm_RiskProfile2Freeze_donuts_by_municipality_wide.svg C:\\Users\\sdfsg\\Documents\\work\\Perm_RiskProfile2Freeze_donuts_by_municipality_wide.png\n",
      "Hail\n",
      "C:\\Users\\sdfsg\\Documents\\work\\Perm_RiskProfile2Hail_donuts_by_municipality_wide.svg C:\\Users\\sdfsg\\Documents\\work\\Perm_RiskProfile2Hail_donuts_by_municipality_wide.png\n",
      "Thunder\n",
      "C:\\Users\\sdfsg\\Documents\\work\\Perm_RiskProfile2Thunder_donuts_by_municipality_wide.svg C:\\Users\\sdfsg\\Documents\\work\\Perm_RiskProfile2Thunder_donuts_by_municipality_wide.png\n",
      "Wildfire\n",
      "C:\\Users\\sdfsg\\Documents\\work\\Perm_RiskProfile2Wildfire_donuts_by_municipality_wide.svg C:\\Users\\sdfsg\\Documents\\work\\Perm_RiskProfile2Wildfire_donuts_by_municipality_wide.png\n",
      "Flood\n",
      "C:\\Users\\sdfsg\\Documents\\work\\Perm_RiskProfile2Flood_donuts_by_municipality_wide.svg C:\\Users\\sdfsg\\Documents\\work\\Perm_RiskProfile2Flood_donuts_by_municipality_wide.png\n",
      "Drought\n",
      "C:\\Users\\sdfsg\\Documents\\work\\Perm_RiskProfile2Drought_donuts_by_municipality_wide.svg C:\\Users\\sdfsg\\Documents\\work\\Perm_RiskProfile2Drought_donuts_by_municipality_wide.png\n",
      "Economy\n"
     ]
    }
   ],
   "source": [
    "econ_fc = hazard_data[\"Economy\"]\n",
    "\n",
    "# ---------- paths & input ----------\n",
    "subfolder = f'{base_dir}/out_with_pct/'\n",
    "for hazard_name in hazard_data.keys():\n",
    "    print(hazard_name)\n",
    "    if hazard_name =='Economy':\n",
    "        continue\n",
    "    else: risk = hazard_name\n",
    "    in_path = f\"{subfolder}{risk}_withPct.csv\"\n",
    "\n",
    "    # ---------- load (ignore geometry to avoid multipart errors) ----------\n",
    "    gdf = pd.read_csv(in_path)\n",
    "\n",
    "    # ---------- config ----------\n",
    "    # Category order (0..4) maps to danger levels below\n",
    "    category_order = [\"неопасно\", \"умерено опасно\", \"опасно\", \"весьма опасно\", \"очень опасно\"]\n",
    "    category_colors = {\n",
    "        \"неопасно\": \"#a5de94\",\n",
    "        \"умерено опасно\": \"#feea80\",\n",
    "        \"опасно\": \"#fec979\",\n",
    "        \"весьма опасно\": \"#fb8066\",\n",
    "        \"очень опасно\": \"#f65356\"\n",
    "    }\n",
    "    cat_idx_to_name = {i: name for i, name in enumerate(category_order)}\n",
    "\n",
    "    # Assets to try to render (will be filtered by availability)\n",
    "    all_assets = [\"ho\", \"ro\", \"fo\", \"ag\", \"pop\"]\n",
    "\n",
    "    # Absolute totals for the donut center\n",
    "    center_value_cols = {\n",
    "        \"ho\":  \"rViHA_ho\",\n",
    "        \"ro\":  \"rViHA_ro\",\n",
    "        \"fo\":  \"rViHA_fo\",\n",
    "        \"ag\":  \"rViHA_ag\",\n",
    "        \"pop\": \"rNiHA_pop\",  # people\n",
    "    }\n",
    "\n",
    "    if hazard_name =='Wildfire':\n",
    "        center_value_cols = {\n",
    "        \"ho\":  \"rVaR_ho\",\n",
    "        \"ro\":  \"rVaR_ro\",\n",
    "        \"fo\":  \"rVaR_fo\",\n",
    "        \"ag\":  \"rVaR_ag\",\n",
    "        \"pop\": \"rNaR_pop\",  # people\n",
    "    }\n",
    "\n",
    "    # Detect municipality column\n",
    "    mo_col = \"MO\" if \"MO\" in gdf.columns else None\n",
    "    if mo_col is None:\n",
    "        for cand in (\"MO_NAME\", \"municipality\", \"МО\"):\n",
    "            if cand in gdf.columns:\n",
    "                mo_col = cand\n",
    "                break\n",
    "    assert mo_col is not None, \"Не найден столбец с муниципалитетом (например, 'MO').\"\n",
    "\n",
    "    # ✅ Use the Economy DataFrame here (NOT the FC path string)\n",
    "    key = pick_join_key(gdf, eco)                 # was: pick_join_key(gdf, econ_fc)\n",
    "    eco_tmp = eco[[key] + list(price_cols.values())].copy()  # was: econ_fc[[...]]\n",
    "    eco_tmp[\"_join_key\"] = norm_key(eco_tmp[key])\n",
    "    eco_idx = eco_tmp.set_index(\"_join_key\")\n",
    "\n",
    "    # one lookup Series per asset, indexed by normalized join key\n",
    "    mo_price_lookup = {\n",
    "        a: (eco_idx[price_cols[a]] if price_cols[a] in eco_idx.columns else pd.Series(dtype=float))\n",
    "        for a in [\"ho\",\"ro\",\"fo\",\"ag\",\"pop\"]\n",
    "    }\n",
    "\n",
    "    # population too, if you want to center-show pop totals sometimes\n",
    "    mo_price_lookup[\"pop\"] = eco_idx.get(\"popTotal\", pd.Series(dtype=float))\n",
    "\n",
    "    # ---------- figure out which assets are fully available ----------\n",
    "    present_assets = []\n",
    "    pct_cols_by_asset = {}\n",
    "    for a in all_assets:\n",
    "        pct_cols = [f\"{a}_pct_{i}\" for i in range(5)]\n",
    "        if all(col in gdf.columns for col in pct_cols) and (center_value_cols[a] in gdf.columns):\n",
    "            present_assets.append(a)\n",
    "            pct_cols_by_asset[a] = pct_cols\n",
    "\n",
    "    if not present_assets:\n",
    "        raise RuntimeError(\"Нет ни одного актива с полным набором *_pct_0..4 и абсолютным столбцом для центра.\")\n",
    "\n",
    "    # ---------- plotting ----------\n",
    "    mos = sorted(gdf[mo_col].astype(str).unique().tolist())\n",
    "    n_rows, n_cols = len(mos), len(present_assets)\n",
    "\n",
    "    plt.rcParams['svg.fonttype'] = 'none'\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(22, max(3.0, 2.2 * n_rows)))\n",
    "\n",
    "    # normalize axes to 2D indexable array\n",
    "    if n_rows == 1 and n_cols == 1:\n",
    "        axes = np.array([[axes]])\n",
    "    elif n_rows == 1:\n",
    "        axes = np.array([axes])\n",
    "    elif n_cols == 1:\n",
    "        axes = np.array([[ax] for ax in axes])\n",
    "\n",
    "    # set column titles (assets) only on the top row\n",
    "    for c, a in enumerate(present_assets):\n",
    "        axes[0][c].set_title(a, fontsize=13)\n",
    "\n",
    "    for r, mo in enumerate(mos):\n",
    "        df_mo = gdf[gdf[mo_col].astype(str) == mo]\n",
    "\n",
    "        for c, a in enumerate(present_assets):\n",
    "            ax = axes[r][c]\n",
    "\n",
    "            # municipality key for Economy lookup\n",
    "            mo_key_norm = norm_key(pd.Series([df_mo[mo_col].iloc[0]])).iloc[0]\n",
    "            center_val = mo_price_lookup[a].get(mo_key_norm, np.nan)\n",
    "\n",
    "            # --- CASE A: no assets in this muni → gray ring, no %\n",
    "            no_assets = (not np.isfinite(center_val)) or (float(center_val) <= 0)\n",
    "            if no_assets:\n",
    "                ax.pie([1.0], labels=None, colors=[\"#e0e0e0\"], startangle=90,\n",
    "                    counterclock=True, wedgeprops=dict(width=0.5))\n",
    "                ax.text(0, 0, \"\", ha=\"center\", va=\"center\", fontsize=11, weight=\"bold\")\n",
    "                continue  # skip share logic\n",
    "\n",
    "            # --- Build shares from *_pct_0..4 ---\n",
    "            cols = pct_cols_by_asset[a]\n",
    "            shares_df = df_mo[cols]\n",
    "\n",
    "            # If upstream wiped pct to NaN for this muni+asset, fall back to gray\n",
    "            if not shares_df.notna().any().any():\n",
    "                ax.pie([1.0], labels=None, colors=[\"#e0e0e0\"], startangle=90,\n",
    "                    counterclock=True, wedgeprops=dict(width=0.5))\n",
    "                ax.text(0, 0, format_center(a, float(center_val)), ha=\"center\", va=\"center\",\n",
    "                        fontsize=11, weight=\"bold\")\n",
    "                continue\n",
    "\n",
    "            shares_series = shares_df.sum(numeric_only=True)\n",
    "            shares_series.index = [cat_idx_to_name[int(col.split(\"_\")[-1])] for col in shares_series.index]\n",
    "            shares_series = shares_series.reindex(category_order, fill_value=0.0)\n",
    "\n",
    "            # --- IMPORTANT: avoid zero-sum pies ---\n",
    "            total_sum = float(np.nansum(shares_series.values))\n",
    "            if not np.isfinite(total_sum) or total_sum <= 0:\n",
    "                # assets exist but no distribution recorded → 100% \"неопасно\"\n",
    "                shares_series.loc[:] = 0.0\n",
    "                shares_series[\"неопасно\"] = 100.0\n",
    "            elif total_sum < 100:\n",
    "                # top-up remainder into \"неопасно\"\n",
    "                shares_series[\"неопасно\"] += (100 - total_sum)\n",
    "\n",
    "            shares = [float(x) for x in shares_series.tolist()]\n",
    "\n",
    "            # center = municipality’s Economy total (not damage)\n",
    "            ax.text(0, 0, format_center(a, float(center_val)),\n",
    "                    ha=\"center\", va=\"center\", fontsize=11, weight=\"bold\")\n",
    "\n",
    "            colors = [category_colors[k] for k in category_order]\n",
    "            ax.pie(\n",
    "                shares,\n",
    "                labels=None,\n",
    "                autopct=autopct_fmt,   # your smart % formatter\n",
    "                colors=colors,\n",
    "                startangle=90,\n",
    "                counterclock=True,\n",
    "                wedgeprops=dict(width=0.5),\n",
    "                pctdistance=1.2,\n",
    "                textprops=dict(fontsize=16),\n",
    "            )\n",
    "\n",
    "        # --- add y-axis label only once per row ---\n",
    "        axes[r][0].set_ylabel(\n",
    "            mo.replace(\" \", \"\\n\"),   # line break for long names\n",
    "            fontsize=13, weight=\"bold\", rotation=0, labelpad=40, va=\"center\"\n",
    "        )\n",
    "\n",
    "    # layout & save\n",
    "    plt.subplots_adjust(hspace=0.4, wspace=0.25)\n",
    "    plt.tight_layout(rect=[0.04, 0.04, 0.98, 0.96])\n",
    "\n",
    "    Path(base_dir).mkdir(parents=True, exist_ok=True)\n",
    "    svg_path = f\"{base_dir}{risk}_donuts_by_municipality_wide.svg\"\n",
    "    png_path = f\"{base_dir}{risk}_donuts_by_municipality_wide.png\"\n",
    "    plt.savefig(svg_path, bbox_inches=\"tight\")\n",
    "    plt.savefig(png_path, bbox_inches=\"tight\", dpi=180)\n",
    "    plt.show()\n",
    "\n",
    "    print(svg_path, png_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arcgispro-py3-clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
