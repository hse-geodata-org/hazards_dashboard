{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61ff899b",
   "metadata": {},
   "source": [
    "Код расчитан на то, что шейпы называются как в gdb. Нужно менять region и gdb_path. Результаты в папке unified_risks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c47c7dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcpy\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import log10, floor, isfinite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6522f71",
   "metadata": {},
   "source": [
    "Структура файла:\n",
    "OBJECTID, region, MO, risk_hNormTot, risk_hExpertTot, risk_hNormLive, risk_hExpertLive, risk_hNormTot_num, risk_hExpertTot_num, risk_hNormLive_num, risk_hExpertLive_num, где вместо risk - каждый из рисков из словаря, _num - числовое значение категории"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a133b832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Поиск feature classes в risk feature dataset...\n",
      "  ✓ Найден: Heat -> Yakutia_hazHeat_MO\n",
      "  ✓ Найден: Freeze -> Yakutia_hazFreez5_MO\n",
      "  ✓ Найден: Drought -> Yakutia_hazDrought_MO\n",
      "  ✓ Найден: Thunder -> Yakutia_hazThunderstorm_MO\n",
      "  ✓ Найден: Hail -> Yakutia_hazHail_MO\n",
      "  ✓ Найден: Precipitation -> Yakutia_hazRain_MO\n",
      "  ✓ Найден: Wind -> Yakutia_hazWind_MO\n",
      "  ✓ Найден: Cold -> Yakutia_hazCold_MO\n",
      "  ✓ Найден: Flood -> Yakutia_hazFlood_MO\n",
      "  ✓ Найден: Permafrost -> Yakutia_hazPermafrost_MO\n",
      "  ✓ Найден: Wildfire -> Yakutia_hazWildfire_MO\n",
      "\n",
      "Всего найдено рисков: 11\n",
      "Чтение данных: Heat\n",
      "  Доступные колонки риска: ['hNormTot', 'hExpertTot', 'hNormLive', 'hExpertLive']\n",
      "  Прочитано записей: 36\n",
      "Чтение данных: Freeze\n",
      "  Доступные колонки риска: ['hNormTot', 'hExpertTot', 'hNormLive', 'hExpertLive']\n",
      "  Прочитано записей: 36\n",
      "Чтение данных: Drought\n",
      "  Доступные колонки риска: ['hNormTot', 'hExpertTot', 'hNormLive', 'hExpertLive']\n",
      "  Прочитано записей: 36\n",
      "Чтение данных: Thunder\n",
      "  Доступные колонки риска: ['hNormTot', 'hExpertTot', 'hNormLive', 'hExpertLive']\n",
      "  Прочитано записей: 36\n",
      "Чтение данных: Hail\n",
      "  Доступные колонки риска: ['hNormTot', 'hExpertTot', 'hNormLive', 'hExpertLive']\n",
      "  Прочитано записей: 36\n",
      "Чтение данных: Precipitation\n",
      "  Доступные колонки риска: ['hNormTot', 'hExpertTot', 'hNormLive', 'hExpertLive']\n",
      "  Прочитано записей: 36\n",
      "Чтение данных: Wind\n",
      "  Доступные колонки риска: ['hNormTot', 'hExpertTot', 'hNormLive', 'hExpertLive']\n",
      "  Прочитано записей: 36\n",
      "Чтение данных: Cold\n",
      "  Доступные колонки риска: ['hNormTot', 'hExpertTot', 'hNormLive', 'hExpertLive']\n",
      "  Прочитано записей: 36\n",
      "Чтение данных: Flood\n",
      "  Доступные колонки риска: ['hNormTot', 'hExpertTot', 'hNormLive', 'hExpertLive']\n",
      "  Прочитано записей: 36\n",
      "Чтение данных: Permafrost\n",
      "  Доступные колонки риска: ['hNormTot', 'hExpertTot', 'hNormLive', 'hExpertLive']\n",
      "  Прочитано записей: 36\n",
      "Чтение данных: Wildfire\n",
      "  Доступные колонки риска: ['hNormTot', 'hExpertTot', 'hNormLive', 'hExpertLive']\n",
      "  Прочитано записей: 36\n",
      "\n",
      "Объединенная таблица: 396 записей\n",
      "Преобразование в широкий формат...\n",
      "Широкий формат: 36 муниципальных районов\n",
      "✓ Объединенная таблица сохранена: C:\\Users\\sdfsg\\Documents\\work\\data\\gispy\\perm\\unified_risks\\Yakutia_unified_risks_data.csv\n",
      "✓ Информация о рисках сохранена: C:\\Users\\sdfsg\\Documents\\work\\data\\gispy\\perm\\unified_risks\\Yakutia_risks_info.csv\n",
      "\n",
      "============================================================\n",
      "ОБРАБОТКА ТАБЛИЦ ЗАВЕРШЕНА!\n",
      "Выходная директория: C:\\Users\\sdfsg\\Documents\\work\\data\\gispy\\perm\\unified_risks\n",
      "Муниципальных районов: 36\n",
      "Рисков: 11\n",
      "\n",
      "Первые 10 районов с новыми ID:\n",
      "  1: Абыйский муниципальный район\n",
      "  2: Алданский муниципальный район\n",
      "  3: Аллаиховский муниципальный район\n",
      "  4: Амгинский муниципальный район\n",
      "  5: Анабарский национальный (долгано-эвенкийский) муниципальный район\n",
      "  6: Булунский муниципальный район\n",
      "  7: Верхневилюйский муниципальный район\n",
      "  8: Верхнеколымский муниципальный район\n",
      "  9: Верхоянский муниципальный район\n",
      "  10: Вилюйский муниципальный район\n",
      "\n",
      "Пример числовых значений:\n",
      "  Heat_hNormTot_num: [0, 1, 2, 3, 4]\n",
      "  Heat_hExpertTot_num: [0, 1]\n",
      "  Heat_hNormLive_num: [0, 1, 2, 3, 4]\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ========= PATHS =========\n",
    "region = 'Yakutia'\n",
    "gdb_path = r\"C:\\Users\\sdfsg\\Downloads\\Yakutia_RiskProfile_10-10-2025\\Yakutia_RiskProfile_08-10-2025\\Yakutia_RiskProfile\\Default.gdb\"\n",
    "RISKS_FDS = \"risk\"\n",
    "output_base = r\"C:\\Users\\sdfsg\\Documents\\work\\data\\gispy\\perm\"\n",
    "output_dir = Path(output_base) / \"unified_risks\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ========= MAPPING & CONFIG =========\n",
    "hazard_mapping = {\n",
    "    \"hazCold\": \"Cold\",\n",
    "    \"hazDrought\": \"Drought\", \n",
    "    \"hazWildfire\": \"Wildfire\",\n",
    "    \"hazFlood\": \"Flood\",\n",
    "    \"hazFreez5\": \"Freeze\",\n",
    "    \"hazHail\": \"Hail\",\n",
    "    \"hazHeat\": \"Heat\",\n",
    "    \"hazRain\": \"Precipitation\",\n",
    "    \"hazThunderstorm\": \"Thunder\",\n",
    "    \"hazWind\": \"Wind\",\n",
    "    \"hazPermafrost\": \"Permafrost\",\n",
    "}\n",
    "\n",
    "# Точный словарь для перевода категорий\n",
    "category_mapping = {\n",
    "    \"неопасно\": 0,\n",
    "    \"умерено опасно\": 1,\n",
    "    \"опасно\": 2,\n",
    "    \"весьма опасно\": 3,\n",
    "    \"очень опасно\": 4,\n",
    "    \"не опасно\": 0,\n",
    "    \"умеренно опасно\": 1,\n",
    "}\n",
    "\n",
    "# Колонки для преобразования\n",
    "risk_columns = [\"hNormTot\", \"hExpertTot\", \"hNormLive\", \"hExpertLive\"]\n",
    "base_columns = [\"region\", \"MO\"]  # Убрали OBJECTID из базовых колонок\n",
    "\n",
    "# ========= HELPERS =========\n",
    "def extract_short_code(fc_name: str) -> str:\n",
    "    \"\"\"Извлекает код опасности из названия feature class.\"\"\"\n",
    "    m = re.search(r\"(haz[A-Za-z0-9]+)\", fc_name)\n",
    "    if m:\n",
    "        return m.group(1)\n",
    "    for key in hazard_mapping.keys():\n",
    "        if key.lower() in fc_name.lower():\n",
    "            return key\n",
    "    return fc_name\n",
    "\n",
    "def convert_category_to_numeric(category):\n",
    "    \"\"\"Точно преобразует текстовую категорию в числовое значение.\"\"\"\n",
    "    if category is None:\n",
    "        return None\n",
    "    category_str = str(category).strip()\n",
    "    for text_cat, num_cat in category_mapping.items():\n",
    "        if text_cat == category_str:\n",
    "            return num_cat\n",
    "    return None\n",
    "\n",
    "def get_feature_classes():\n",
    "    \"\"\"Собирает все feature classes с рисками.\"\"\"\n",
    "    hazard_data = {}\n",
    "    \n",
    "    arcpy.env.workspace = os.path.join(gdb_path, RISKS_FDS)\n",
    "    print(\"Поиск feature classes в risk feature dataset...\")\n",
    "    \n",
    "    for fc in arcpy.ListFeatureClasses() or []:\n",
    "        name_l = fc.lower()\n",
    "        if \"template\" in name_l or \"perm_hazardtemplate_mo\" in name_l:\n",
    "            continue\n",
    "        \n",
    "        short = extract_short_code(fc)\n",
    "        if short in hazard_mapping:  # Используем только риски из mapping, исключаем economics\n",
    "            hazard_name = hazard_mapping[short]\n",
    "            full_path = os.path.join(gdb_path, RISKS_FDS, fc)\n",
    "            hazard_data[hazard_name] = full_path\n",
    "            print(f\"  ✓ Найден: {hazard_name} -> {fc}\")\n",
    "    \n",
    "    print(f\"\\nВсего найдено рисков: {len(hazard_data)}\")\n",
    "    return hazard_data\n",
    "\n",
    "def read_risk_data(fc_path, risk_name):\n",
    "    \"\"\"Читает данные риска и преобразует категории.\"\"\"\n",
    "    print(f\"Чтение данных: {risk_name}\")\n",
    "    \n",
    "    # Получаем все поля\n",
    "    all_fields = [f.name for f in arcpy.ListFields(fc_path)]\n",
    "    \n",
    "    # Проверяем наличие базовых колонок (без OBJECTID)\n",
    "    missing_base = [col for col in base_columns if col not in all_fields]\n",
    "    if missing_base:\n",
    "        print(f\"  ⚠️ Отсутствуют базовые колонки: {missing_base}\")\n",
    "        return None\n",
    "    \n",
    "    # Определяем какие колонки риска есть в файле\n",
    "    available_risk_cols = [col for col in risk_columns if col in all_fields]\n",
    "    print(f\"  Доступные колонки риска: {available_risk_cols}\")\n",
    "    \n",
    "    # Собираем все нужные поля (без OBJECTID)\n",
    "    fields_to_read = base_columns + available_risk_cols\n",
    "    data = []\n",
    "    \n",
    "    with arcpy.da.SearchCursor(fc_path, fields_to_read) as cursor:\n",
    "        for row in cursor:\n",
    "            record = dict(zip(fields_to_read, row))\n",
    "            \n",
    "            # Добавляем название риска\n",
    "            record['risk_name'] = risk_name\n",
    "            \n",
    "            # Преобразуем категориальные колонки в числовые\n",
    "            for col in available_risk_cols:\n",
    "                numeric_val = convert_category_to_numeric(record[col])\n",
    "                if numeric_val is not None:\n",
    "                    record[f\"{col}_num\"] = numeric_val\n",
    "            \n",
    "            data.append(record)\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    print(f\"  Прочитано записей: {len(df)}\")\n",
    "    return df\n",
    "\n",
    "def create_unified_dataframe(hazard_data):\n",
    "    \"\"\"Создает объединенную таблицу со всеми рисками.\"\"\"\n",
    "    all_data = []\n",
    "    \n",
    "    for risk_name, fc_path in hazard_data.items():\n",
    "        df = read_risk_data(fc_path, risk_name)\n",
    "        if df is not None and len(df) > 0:\n",
    "            all_data.append(df)\n",
    "    \n",
    "    if not all_data:\n",
    "        raise Exception(\"Не удалось прочитать данные ни по одному риску\")\n",
    "    \n",
    "    # Объединяем все данные\n",
    "    unified_df = pd.concat(all_data, ignore_index=True)\n",
    "    print(f\"\\nОбъединенная таблица: {len(unified_df)} записей\")\n",
    "    \n",
    "    return unified_df\n",
    "\n",
    "def reshape_to_wide_format(df):\n",
    "    \"\"\"Преобразует данные в широкий формат (один MO = одна строка).\"\"\"\n",
    "    print(\"Преобразование в широкий формат...\")\n",
    "    \n",
    "    # Группируем по MO и создаем широкий формат\n",
    "    wide_data = []\n",
    "    \n",
    "    # Сортируем муниципальные районы по алфавиту для последовательной нумерации\n",
    "    sorted_mos = sorted(df['MO'].unique())\n",
    "    \n",
    "    for obj_id, mo in enumerate(sorted_mos, 1):  # Начинаем с 1\n",
    "        mo_data = df[df['MO'] == mo]\n",
    "        \n",
    "        if len(mo_data) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Берем первую запись для базовых колонок\n",
    "        base_record = mo_data.iloc[0][base_columns].to_dict()\n",
    "        record = base_record.copy()\n",
    "        \n",
    "        # Добавляем новый OBJECTID\n",
    "        record['OBJECTID'] = obj_id\n",
    "        \n",
    "        # Добавляем данные по каждому риску\n",
    "        for risk in df['risk_name'].unique():\n",
    "            risk_data = mo_data[mo_data['risk_name'] == risk]\n",
    "            \n",
    "            if len(risk_data) > 0:\n",
    "                risk_row = risk_data.iloc[0]\n",
    "                for col in risk_columns:\n",
    "                    # Оригинальные текстовые значения\n",
    "                    record[f\"{risk}_{col}\"] = risk_row.get(col)\n",
    "                    # Числовые значения - преобразуем в целые числа\n",
    "                    num_val = risk_row.get(f\"{col}_num\")\n",
    "                    if num_val is not None and not pd.isna(num_val):\n",
    "                        record[f\"{risk}_{col}_num\"] = int(num_val)\n",
    "                    else:\n",
    "                        record[f\"{risk}_{col}_num\"] = None\n",
    "            else:\n",
    "                # Если данных по этому риску нет, заполняем None\n",
    "                for col in risk_columns:\n",
    "                    record[f\"{risk}_{col}\"] = None\n",
    "                    record[f\"{risk}_{col}_num\"] = None\n",
    "        \n",
    "        wide_data.append(record)\n",
    "    \n",
    "    wide_df = pd.DataFrame(wide_data)\n",
    "    \n",
    "    # Переупорядочиваем колонки, чтобы OBJECTID был первой\n",
    "    cols = ['OBJECTID'] + base_columns + [col for col in wide_df.columns if col not in ['OBJECTID'] + base_columns]\n",
    "    wide_df = wide_df[cols]\n",
    "    \n",
    "    # Преобразуем все числовые колонки в целый тип\n",
    "    for col in wide_df.columns:\n",
    "        if col.endswith('_num') and wide_df[col].dtype in ['float64', 'float32']:\n",
    "            wide_df[col] = wide_df[col].astype('Int64')  # Int64 поддерживает NaN\n",
    "    \n",
    "    print(f\"Широкий формат: {len(wide_df)} муниципальных районов\")\n",
    "    \n",
    "    return wide_df\n",
    "\n",
    "# ========= ОСНОВНОЙ КОД =========\n",
    "def main():\n",
    "    # Собираем feature classes\n",
    "    hazard_data = get_feature_classes()\n",
    "    \n",
    "    if not hazard_data:\n",
    "        print(\"Не найдено feature classes с рисками!\")\n",
    "        return\n",
    "    \n",
    "    # Создаем объединенную таблицу\n",
    "    unified_df = create_unified_dataframe(hazard_data)\n",
    "    \n",
    "    # Преобразуем в широкий формат\n",
    "    wide_df = reshape_to_wide_format(unified_df)\n",
    "    \n",
    "    # Сохраняем объединенную таблицу с припиской региона\n",
    "    table_filename = f\"{region}_unified_risks_data.csv\"\n",
    "    table_path = output_dir / table_filename\n",
    "    wide_df.to_csv(table_path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"✓ Объединенная таблица сохранена: {table_path}\")\n",
    "    \n",
    "    # Дополнительно: сохраняем информацию о рисках\n",
    "    risks_info = pd.DataFrame({\n",
    "        'risk_name': list(hazard_data.keys()),\n",
    "        'feature_class': [os.path.basename(path) for path in hazard_data.values()]\n",
    "    })\n",
    "    risks_info_path = output_dir / f\"{region}_risks_info.csv\"\n",
    "    risks_info.to_csv(risks_info_path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"✓ Информация о рисках сохранена: {risks_info_path}\")\n",
    "    \n",
    "    # Выводим информацию о результате\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(\"ОБРАБОТКА ТАБЛИЦ ЗАВЕРШЕНА!\")\n",
    "    print(f\"Выходная директория: {output_dir}\")\n",
    "    print(f\"Муниципальных районов: {len(wide_df)}\")\n",
    "    print(f\"Рисков: {len(hazard_data)}\")\n",
    "    \n",
    "    print(\"\\nПервые 10 районов с новыми ID:\")\n",
    "    for i, (_, row) in enumerate(wide_df.head(10).iterrows(), 1):\n",
    "        print(f\"  {row['OBJECTID']}: {row['MO']}\")\n",
    "    \n",
    "    print(\"\\nПример числовых значений:\")\n",
    "    num_cols = [col for col in wide_df.columns if col.endswith('_num')]\n",
    "    for col in num_cols[:3]:  # Покажем первые 3 числовых колонки\n",
    "        unique_vals = wide_df[col].dropna().unique()\n",
    "        print(f\"  {col}: {sorted(unique_vals)}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arcgispro-py3-clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
